---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# embeddingplyr: Tools for Working With Text Embeddings

<!-- badges: start -->
<!-- badges: end -->

## About

embeddingplyr enables common operations with word and text embeddings within a 'tidyverse'/'quanteda' workflow, as demonstrated in [Data Science for Psychology: Natural Language](http://ds4psych.com). It includes simple functions for calculating common similarity metrics, as well as higher level functions for loading pretrained word embedding models (e.g. [GloVe](https://nlp.stanford.edu/projects/glove/)), applying them to words, aggregating to produce text embeddings, reducing dimensionality, and more.

## Installation

You can install the development version of embeddingplyr from [GitHub](https://github.com/) with:

``` r
# devtools package required to install quanteda from Github 
remotes::install_github("rimonim/embeddingplyr") 
```

## Functionality

embeddingplyr is designed to facilitate the use of word and text embeddings in common data manipulation and text analysis workflows, without introducing new syntax or unfamiliar data structures.

embeddingplyr is model agnostic; it can be used to work with embeddings from decontextualized models like [GloVe](https://nlp.stanford.edu/projects/glove/) and [word2vec](https://code.google.com/archive/p/word2vec/), or from contextualized models like BERT or others made available through the '[text](https://r-text.org)' package.

### Loading Pretrained Embeddings

embeddingplyr won't help you train new embedding models, but it can load embeddings from a file. This is especially useful for pretrained word embedding models like GloVe and word2vec. These models can be downloaded (generally in txt or bin format, respectively) and loaded with `load_embeddings()`.

```{r, warning=FALSE, message=FALSE}
library(embeddingplyr)

glove_twitter_25d <- load_embeddings("~/Documents/data/glove/glove.twitter.27B.25d.txt")
```

The outcome is an embeddings object. An embeddings object is just a numeric matrix with tokens as rownames. This means that it can be easily coerced to a dataframe or tibble, while also allowing special embeddings-specific methods and functions, such as `predict.embeddings()` and `find_nearest()`:

```{r}
moral_embeddings <- predict(glove_twitter_25d, c("good", "bad"))
moral_embeddings

find_nearest(glove_twitter_25d, "dog", 5L, sim_func = cos_sim)
```

### Similarity Metrics

Functions for similarity and distance metrics are as simple as possible; each one takes in vectors and outputs a scalar.

```{r}
vec1 <- c(1, 5, 2)
vec2 <- c(4, 2, 2)
vec3 <- c(-1, -2, -13)

dot_prod(vec1, vec2)            		    # dot product
cos_sim(vec1, vec2)             		    # cosine similarity
euc_dist(vec1, vec2)            		    # Euclidean distance
anchored_sim(vec1, pos = vec2, neg = vec3)  # projection to an anchored vector
```

### Example Tidy Workflow

Given a tidy dataframe of texts, `embed_docs()` will generate embeddings by averaging the embeddings of words in each text (for more information on why this works well, see [Data Science for Psychology, Chapter 18](https://ds4psych.com/decontextualized-embeddings#sec-embedding-magnitude)). 

```{r, warning=FALSE, message=FALSE}
library(dplyr)
valence_df <- tribble(
	~id,        ~text,
	"positive", "happy awesome cool nice",
	"neutral",  "ok fine sure whatever",
	"negative", "sad bad horrible angry"
	)

valence_embeddings_df <- valence_df |> 
	embed_docs("text", glove_twitter_25d, id_col = "id", .keep_all = TRUE)
valence_embeddings_df
```

`embed_docs()` can also be used to generate other types of embeddings. For example, we can use the '[text](https://r-text.org)' package to generate embeddings using any model available from Huggingface transformers.

```{r, eval=FALSE}
sbert_embeddings <- function(texts) {
	text::textEmbed(
	  texts,
	  model = "sentence-transformers/all-MiniLM-L12-v2", # model name
	  layers = -2,  # second to last layer (default)
	  tokens_select = "[CLS]", # use only [CLS] token
	  dim_name = FALSE,
	  keep_token_embeddings = FALSE
  )$texts[[1]]
}

valence_sbert_df <- valence_df |> 
	embed_docs("text", sbert_embeddings, id_col = "id", .keep_all = TRUE)
```

To quantify how good and how intense the texts are, we can compare them to the embeddings for "good" and "intense" using `get_similarities()`. Note that this step requires only a dataframe,  tibble, or embeddings object with numeric columns; the embeddings can come from any source. 

```{r}
good_vec <- predict(glove_twitter_25d, "good")
intense_vec <- predict(glove_twitter_25d, "intense")
valence_quantified <- valence_embeddings_df |> 
	get_similarities(
		dim_1:dim_25, 
		list(
			good = good_vec, 
			intense = intense_vec
			)
		)
valence_quantified
```
### Example Quanteda Workflow

```{r}
library(quanteda)

valence_corp <- corpus(valence_df, docid_field = "id")
valence_corp

valence_dfm <- valence_corp |> 
	tokens() |> 
	dfm()

valence_embeddings_df <- valence_dfm |> 
	textstat_embedding(glove_twitter_25d)
valence_embeddings_df
```

### Other Functions

#### Reduce Dimensionality

It is sometimes useful to reduce the dimensionality of embeddings. This is done with `reduce_dimensionality()`, which by default performs PCA without column normalization.

```{r}
valence_df_2d <- valence_embeddings_df |> 
	reduce_dimensionality(dim_1:dim_25, 2)
valence_df_2d
```
#### Normalize (Scale Embeddings to the Unit Hypersphere)

```{r}
normalize(good_vec)

normalize(moral_embeddings)

valence_embeddings_df |> normalize_rows(dim_1:dim_25)
```

#### Magnitude

```{r}
magnitude(good_vec)

magnitude(moral_embeddings)
```
